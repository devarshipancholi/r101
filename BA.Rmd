---
title: "BA HW 6"
author: "Devarshi Pancholi"
date: "10/31/2019"
output: pdf_document
---

***Problem 1: Student Application Data(Redo)***

```{r include=FALSE}

stu <- na.omit(read.csv("https://raw.githubusercontent.com/jcbonilla/BusinessAnalytics/master/BAData/Univ%20Admissions.csv", header = TRUE, stringsAsFactors = TRUE, na.strings = c("", "NA")))
#head(stu,10)
#View(stu)

stu[!(is.na(stu$x.Country) | stu$x.Country==""), ]
stu[!(is.na(stu$x.State) | stu$x.State==""), ]
stu[!(is.na(stu$x.Gender) | stu$x.Gender==""), ]
stu[!(is.na(stu$x.Source) | stu$x.Source==""), ]
stu[!(is.na(stu$x.GPA) | stu$x.GPA==""), ]
stu[!(is.na(stu$x.DistancetoCampus_miles) | stu$x.DistancetoCampus_miles==""), ]
stu[!(is.na(stu$x.HouseholdIncome) | stu$x.HouseholdIncome==""), ]
stu[!(is.na(stu$x.Status.1) | stu$x.Status.1==""), ]
stu[!(is.na(stu$x.InState) | stu$x.InState==""), ]

str(stu)


```
```{r}

table(stu$x.Status.1)
prop.table(table(stu$x.Status.1))
#barplot(table(stu$x.Status.1))

stu$APPLICANT <- as.logical(0)
stu$PROSPECT <- as.logical(0)
stu$SUSPECT <- as.logical(0)

for(i in 1:nrow(stu)) {
  if (stu$x.Status.1[i]=="APPLICANT")
    stu$APPLICANT[i] <- as.logical(1)
  else if(stu$x.Status.1[i]=="PROSPECT")
    stu$PROSPECT[i] <- as.logical(1)
  else
    stu$SUSPECT[i] <- as.logical(1)
}

#barplot(table(stu$APPLICANT))
View(stu)

library(dplyr)
admit <- select(stu, x.State, x.Gender, x.Source, x.GPA, x.DistancetoCampus_miles, x.HouseholdIncome, x.InState, APPLICANT)

admitlr <- select(stu, x.State, x.Gender, x.GPA, x.DistancetoCampus_miles, x.HouseholdIncome, x.InState, APPLICANT)
str(admit)
```

**1. Run 3 model to predict if a student will apply to university or not.**\newline
**2. Create 90:10 split and validate those models using ratio of correct predictions vs total predictions.**\newline
**3. Create 70:30 split and validate those models using ratio of correct predictions vs total predictions.**\newline
**4. Asses those 3 model using performance metrics such as accuracy, precision, recall, F-score and G-score.**\newline

*First, I will build 3 models with 70:30 split. Then I will evaluate them according to the Question 4 and I also will be plotting AUC curve for me to decide up on a particular model*

```{r}

target <- ('APPLICANT')
dependent <- (names(admit)[names(admit) != target])
dependentlr <- (names(admitlr)[names(admitlr) != target])

admit$APPLICANT<-as.factor(admit$APPLICANT)

library(ROSE)

set.seed(1234)
split <- (.70)
library (caret)
library(kernlab)
library(xgboost)
index <- createDataPartition(admit$APPLICANT, p=split, list=FALSE)

train.df <- admit[ index,]
test.df <- admit[ -index,]

train.under<-ovun.sample(APPLICANT ~., data = train.df, method = "under", N= 1000)$data
prop.table(table(train.under$APPLICANT))

fitControl <- trainControl(method = "none")

lr <- train(train.df[,dependentlr],train.df[,target], method='glm', trControl=fitControl)

rf <- (train(train.under[,dependent],train.under[,target], method='rf', trControl=fitControl))

gbm <- train(train.under[,dependent],train.under[,target], method='gbm', trControl=fitControl)

```

```{r}

lr.predict <- predict(lr,test.df[,dependent],type="raw")
confusionMatrix(lr.predict,test.df[,target], positive = "TRUE")
p <- data.frame(Actual = test.df$APPLICANT , Prediction = lr.predict)
p <- table(p)
p

accuracy <- (p[1,1] + p[2,2])/sum(p)
accuracy

precision <- (p[2,2]/(p[2,2] + p[1,2]))
precision

recall <- (p[2,2]/(p[2,2] + p[2,1]))
recall

f_score <- 2*((precision*recall)/(precision+recall))
f_score

g_score <- sqrt(precision*recall)
g_score

```

```{r}

rf.predict<-predict(rf,test.df[,dependent],type="raw")
confusionMatrix(rf.predict,test.df[,target], positive = "TRUE")
q <- data.frame(Actual = test.df$APPLICANT , Prediction = rf.predict)
q <- table(q)
q

accuracy <- (q[1,1] + q[2,2])/sum(q)
accuracy

precision <- (q[2,2]/(q[2,2] + q[1,2]))
precision

recall <- (q[2,2]/(q[2,2] + q[2,1]))
recall

f_score <- 2*((precision*recall)/(precision+recall))
f_score

g_score <- sqrt(precision*recall)
g_score

```

```{r echo= FALSE}

gbm.predict <- predict(gbm,test.df[,dependent],type="raw")
confusionMatrix(gbm.predict,test.df[,target], positive = "TRUE")
r <- data.frame(Actual = test.df$APPLICANT , Prediction = gbm.predict)
r <- table(r)
r

accuracy <- (r[1,1] + r[2,2])/sum(r)
accuracy

precision <- (r[2,2]/(r[2,2] + r[1,2]))
precision

recall <- (r[2,2]/(r[2,2] + r[2,1]))
recall

f_score <- 2*((precision*recall)/(precision+recall))
f_score

g_score <- sqrt(precision*recall)
g_score

```
```{r}

library(pROC)
gbm.probs <- predict(gbm,test.df[,dependent],type="prob")    
rf.probs <- predict(rf,test.df[,dependent],type="prob") 

gbm.plot<-plot(roc(test.df$APPLICANT,gbm.probs[,2]))
rf.plot<-lines(roc(test.df$APPLICANT,rf.probs[,2]), col="blue")

```

*Now, we will plot  the same models but with 90:10 split and evaluate those models based on the criteria mentioned in Question 4. ROC curve will also be plotted.*

```{r}

set.seed(1234)
split2 <- (.90)
index2 <- createDataPartition(admit$APPLICANT, p=split2, list=FALSE)

train.df2 <- admit[ index2,]
test.df2 <- admit[ -index2,]

train.under2<-ovun.sample(APPLICANT ~., data = train.df2, method = "under", N= 1000)$data
prop.table(table(train.under2$APPLICANT))

fitControl <- trainControl(method = "none")

lr2 <- train(train.df2[,dependentlr],train.df2[,target], method='glm', trControl=fitControl)

rf2 <- (train(train.under2[,dependent],train.under2[,target], method='rf', trControl=fitControl))

gbm2 <- train(train.under2[,dependent],train.under2[,target], method='gbm', trControl=fitControl)

```

```{r}

lr.predict2 <- predict(lr2,test.df2[,dependent],type="raw")
confusionMatrix(lr.predict2,test.df2[,target], positive = "TRUE")
p2 <- data.frame(Actual = test.df2$APPLICANT , Prediction = lr.predict2)
p2 <- table(p2)
p2

accuracy <- (p2[1,1] + p2[2,2])/sum(p2)
accuracy

precision <- (p2[2,2]/(p2[2,2] + p2[1,2]))
precision

recall <- (p2[2,2]/(p2[2,2] + p2[2,1]))
recall

f_score <- 2*((precision*recall)/(precision+recall))
f_score

g_score <- sqrt(precision*recall)
g_score
```

```{r}

rf.predict2 <- predict(rf2,test.df2[,dependent],type="raw")
confusionMatrix(rf.predict2,test.df2[,target], positive = "TRUE")
q2 <- data.frame(Actual = test.df2$APPLICANT , Prediction = rf.predict2)
q2 <- table(q2)
q2

accuracy <- (q2[1,1] + q2[2,2])/sum(q2)
accuracy

precision <- (q2[2,2]/(q2[2,2] + q2[1,2]))
precision

recall <- (q2[2,2]/(q2[2,2] + q2[2,1]))
recall

f_score <- 2*((precision*recall)/(precision+recall))
f_score

g_score <- sqrt(precision*recall)
g_score

```

```{r}

gbm.predict2 <- predict(gbm2,test.df2[,dependent],type="raw")
confusionMatrix(gbm.predict2,test.df2[,target], positive = "TRUE")
r2 <- data.frame(Actual = test.df2$APPLICANT , Prediction = gbm.predict2)
r2 <- table(r2)
r2

accuracy <- (r2[1,1] + r2[2,2])/sum(r2)
accuracy

precision <- (r2[2,2]/(r2[2,2] + r2[1,2]))
precision

recall <- (r2[2,2]/(r2[2,2] + r2[2,1]))
recall

f_score <- 2*((precision*recall)/(precision+recall))
f_score

g_score <- sqrt(precision*recall)
g_score

```

```{r}

gbm.probs2 <- predict(gbm2,test.df2[,dependent],type="prob")    
rf.probs2 <- predict(rf2,test.df2[,dependent],type="prob") 

gbm.plot2 <- plot(roc(test.df2$APPLICANT,gbm.probs2[,2]))
rf.plot2 <- lines(roc(test.df2$APPLICANT,rf.probs2[,2]), col="blue")

```
```{r}

rfOver <- varImp(rf)
rfOver

```


**5. Select the best model and give actionable recommendations to the marketing department.**

Based on the experimentation above, the results are better for the split 90:10. And in that split gradient boosting model performs better. So that can be considered as the best model. 

Based on the variable importance in that model we can provide marketing strategies.

1). As distance from campus to school is an important factor, we could target specific students who are closer to the campus. Students from the same state as the campus are most likely to come to school.

2). Students whose parents have high household income are more interested in coming to this school. Hence such students shlould be targetted.

